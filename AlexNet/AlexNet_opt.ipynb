{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57875851-81df-4f68-a0ed-78e3f5229330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0507a78c-5f25-4835-a02c-a353e9010ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "train = '/mnt/NewVolume/DUMP/Notebook/xray_analytics_cnn/data/BoneFractureDataset/training'\n",
    "test = '/mnt/NewVolume/DUMP/Notebook/xray_analytics_cnn/data/BoneFractureDataset/testing'\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd6aadd-c743-432e-90b4-0a06ad499a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2be9d8-2a52-4cfd-a063-4176f9c09155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_images(x, y):\n",
    "    return x / 255, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88fafad-064c-4f9e-b7ba-4519c188526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8863 files belonging to 2 classes.\n",
      "Using 7091 files for training.\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.keras.utils.image_dataset_from_directory(train, image_size=(256, 256), labels=\"inferred\", label_mode=\"binary\", validation_split=0.2, subset=\"training\",seed=42)\n",
    "train_data = train_data.map(scale_images)\n",
    "train_size = int(len(train_data))\n",
    "train_data_final = train_data.take(train_size)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991a02d3-e333-4a65-b2c5-35d7925fe334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8863 files belonging to 2 classes.\n",
      "Using 1772 files for validation.\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "val_data = tf.keras.utils.image_dataset_from_directory(train, image_size=(256, 256), labels=\"inferred\", label_mode=\"binary\", validation_split=0.2, subset=\"validation\", seed=42)\n",
    "val_data = val_data.map(scale_images)\n",
    "val_data_final = val_data.take(len(val_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e38a29-fd9b-4ae6-8509-6a6b87533ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509c08d4-244f-4fca-bd75-0281f9c129b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████| 20/20 [35:02<00:00, 105.14s/trial, best loss: -0.9915350079536438]\n",
      "Best hyperparameters:\n",
      "{'activation_function': 0, 'learning_rate': 1, 'neurons': 0, 'optimizer': 2}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [1e-3, 1e-4, 1e-5, 1e-6]),  # Learning rate choice\n",
    "    'optimizer': hp.choice('optimizer', [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]),  # Optimizer choice\n",
    "    'neurons': hp.choice('neurons', [64, 128, 256]),  # Number of neurons in the activation layer\n",
    "    'activation_function': hp.choice('activation_function', ['relu', 'tanh', 'sigmoid'])  # Activation function choice\n",
    "}\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective(params):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=(256, 256, 3)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3, 3)),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), activation='relu', padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(params['neurons'], activation=params['activation_function']),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(params['neurons'], activation=params['activation_function']),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')  # Change the number of neurons and activation function\n",
    "    ])\n",
    "\n",
    "    # Create an optimizer based on the choice in params\n",
    "    optimizer = params['optimizer'](learning_rate=params['learning_rate'])\n",
    "\n",
    "    # Compile the model with the chosen optimizer\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model with early stopping (replace train_data and val_data_final with your data)\n",
    "    history = model.fit(train_data_final, epochs=10, validation_data=val_data_final, verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "    # Calculate the validation accuracy\n",
    "    val_accuracy = np.max(history.history['val_accuracy'])\n",
    "    \n",
    "    # Return the negative accuracy (to be minimized)\n",
    "    return -val_accuracy\n",
    "\n",
    "# Create a Trials object to keep track of the optimization process\n",
    "trials = Trials()\n",
    "\n",
    "# Perform Bayesian optimization to find the best hyperparameters\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
